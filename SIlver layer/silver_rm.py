# -*- coding: utf-8 -*-
"""Silver_RM.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1gBFm07ZGFC39PjbPOVeAXrDS87eHcIlL
"""

# Welcome to your new notebook
# Type here in the cell editor to add code!

df = spark.read.format("csv").option("header","true").load("Files/Bronzebronze_raw_material_purchase/raw_material_purchase.csv")
# df now is a Spark DataFrame containing CSV data from "Files/Bronzebronze_raw_material_purchase/raw_material_purchase.csv".
display(df)

from pyspark.sql.functions import col, trim, upper
from pyspark.sql.types import IntegerType, DoubleType

df_silver = (
    df
    .withColumn("PurchaseID", trim(upper(col("PurchaseID"))))
    .withColumn("PlantID", trim(upper(col("PlantID"))))
    .withColumn("SupplierID", trim(upper(col("SupplierID"))))
    .withColumn("QtyKG",col("QtyKG").cast(DoubleType()))
    .withColumn("RatePerKG",col("RatePerKG").cast(DoubleType()))
    .withColumn("FreightCost",col("FreightCost").cast(DoubleType()))
)

from pyspark.sql.window import Window
from pyspark.sql.functions import row_number, desc

window_ = Window.partitionBy("PurchaseID").orderBy(desc("Date"))

df_silver = (
    df_silver
    .withColumn("rn", row_number().over(window_))
    .filter(col("rn") == 1)
    .drop("rn")
)

df_silver = df_silver.filter(col("QtyKG") > 0)

df_silver = df_silver.withColumn(
    "LandedCostPerKG",
    (col("QtyKG") * col("RatePerKG") + col("FreightCost")) / col("QtyKG")
)

from pyspark.sql.functions import lag
from pyspark.sql.window import Window

window_cost = Window.partitionBy("PlantID", "SupplierID") \
                    .orderBy("Date")

df_silver = df_silver.withColumn(
    "PrevRatePerKG",
    lag("RatePerKG").over(window_cost)
)

from pyspark.sql.functions import when

df_silver = df_silver.withColumn(
    "CostSpikeFlag",
    when(
        col("PrevRatePerKG").isNotNull() &
        (col("RatePerKG") > col("PrevRatePerKG") * 1.2),
        1
    ).otherwise(0)
)

display(df_silver)

silver_path = "abfss://Fabric_Test@onelake.dfs.fabric.microsoft.com/Test_Lakehouse.Lakehouse/Files/Silver Layer Files"
df_silver.write.mode("overwrite") \
    .parquet(f"{silver_path}/silver_RawMaterialPurchase")

df_check = spark.read.parquet(f"{silver_path}/silver_RawMaterialPurchase")
display(df_check)

