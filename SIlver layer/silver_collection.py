# -*- coding: utf-8 -*-
"""Silver_collection.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1xJ9JfKkI2MHuY9qw42u_7XbaCFwy8i16
"""

# Welcome to your new notebook
# Type here in the cell editor to add code!

df = spark.read.format("csv").option("header","true").load("Files/Bronzebronze_collections/collections.csv")
# df now is a Spark DataFrame containing CSV data from "Files/Bronzebronze_collections/collections.csv".
display(df)



df_sales = spark.read.parquet("Files/Silver Layer Files/silver_Sales") \
    .select("SalesID", "InvoiceAmount", "InvoiceDate", "DueDate")
display(df_sales)

from pyspark.sql.functions import col, trim, upper
from pyspark.sql.types import DoubleType

df_silver = (
    df
    .withColumn("CollectionID", trim(upper(col("CollectionID"))))
    .withColumn("CustomerID", trim(upper(col("CustomerID"))))
    .withColumn("InvoiceID", trim(upper(col("InvoiceID"))))
    .withColumn("AmountPaid", col("AmountPaid").cast(DoubleType()))
)

df_silver = df_silver.filter(col("AmountPaid") > 0)

from pyspark.sql.functions import to_date

df_silver = df_silver.withColumn(
    "CollectionDate",
    to_date(col("CollectionDate"), "dd-MM-yyyy")
)

display(df_silver)

df_joined = df_silver.join(
    df_sales,
    df_silver["InvoiceID"] == df_sales["SalesID"],
    "left"
)

from pyspark.sql.functions import datediff

df_joined = df_joined.withColumn(
    "DaysToCollect",
    datediff(col("CollectionDate"), col("InvoiceDate"))
)

display(df_joined)

from pyspark.sql.functions import when

df_joined = df_joined.withColumn(
    "IsDelayed",
    when(col("CollectionDate") > col("DueDate"), 1).otherwise(0)
)

df_joined = df_joined.withColumn(
    "InvoiceStatus",
    when(col("AmountPaid") >= col("InvoiceAmount"), "Paid")
    .when(col("AmountPaid") < col("InvoiceAmount"), "Partial")
    .otherwise("Open")
)

display(df_joined)

silver_path = "abfss://Fabric_Test@onelake.dfs.fabric.microsoft.com/Test_Lakehouse.Lakehouse/Files/Silver Layer Files"
df_joined.write.mode("overwrite") \
    .parquet(f"{silver_path}/silver_collection")

df_check = spark.read.parquet(f"{silver_path}/silver_collection")
display(df_check)

