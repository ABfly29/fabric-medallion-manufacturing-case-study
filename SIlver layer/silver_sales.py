# -*- coding: utf-8 -*-
"""Silver_Sales.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1rTB9Yk4N3lmWEQB9Evh_4hePKkf9orxn
"""

# Welcome to your new notebook
# Type here in the cell editor to add code!



df = spark.read.format("csv").option("header","true").load("Files/Bronzebronze_sales_orders/sales_orders.csv")
# df now is a Spark DataFrame containing CSV data from "Files/Bronzebronze_sales_orders/sales_orders.csv".
display(df)

df.printSchema()

from pyspark.sql.functions import col, trim, upper
from pyspark.sql.types import IntegerType, DoubleType

df_silver = (
    df
    .withColumn("SalesID", trim(upper(col("SalesID"))))
    .withColumn("PlantID", trim(upper(col("PlantID"))))
    .withColumn("CustomerID", trim(upper(col("CustomerID"))))
    .withColumn("UnitsSold", col("UnitsSold").cast(IntegerType()))
    .withColumn("Rate", col("Rate").cast(DoubleType()))
    .withColumn("DiscountPercent", col("DiscountPercent").cast(DoubleType()))
    .withColumn("PaymentTermsDays", col("PaymentTermsDays").cast(IntegerType()))
    # .withColumn("Date", col("Date").cast(int ()))
)



from pyspark.sql.functions import to_date

df_silver = df_silver.withColumn(
    "Date",
    date_format(to_date(col("Date"), "dd-MM-yyyy"), "ddMMyyyy").cast("int")
)

display(df_silver)

from pyspark.sql.functions import col

df_silver = df_silver.withColumn("Date", col("Date").cast("int"))

df_silver.printSchema()

display(df_silver)

df_silver = df_silver.filter(col("UnitsSold") > 0)

df_silver = df_silver.withColumn(
    "NetUnitPrice",
    col("Rate") * (1 - col("DiscountPercent") / 100)
)

df_silver = df_silver.withColumn(
    "InvoiceAmount",
    col("UnitsSold") * col("NetUnitPrice")
)

df_silver = df_silver.withColumn(
    "TaxAmount",
    col("InvoiceAmount") * 0.18
)

df_silver = df_silver.withColumn(
    "TotalInvoiceAmount",
    col("InvoiceAmount") + col("TaxAmount")
)

from pyspark.sql.functions import col, to_date, format_string

df_silver = df_silver.withColumn(
    "InvoiceDate",
    to_date(col("Date").cast("string"), "ddMMyyyy")
)

from pyspark.sql.functions import to_date, expr

df_silver = df_silver.withColumn(
    "InvoiceDate",
    to_date(col("Date"))
)

df_silver = df_silver.withColumn(
    "DueDate",
    expr("date_add(InvoiceDate, PaymentTermsDays)")
)

from pyspark.sql.functions import when

df_silver = df_silver.withColumn(
    "HighDiscountFlag",
    when(col("DiscountPercent") > 30, 1).otherwise(0)
)

silver_path = "abfss://Fabric_Test@onelake.dfs.fabric.microsoft.com/Test_Lakehouse.Lakehouse/Files/Silver Layer Files"
df_silver.write.mode("overwrite") \
    .parquet(f"{silver_path}/silver_Sales")

df_check = spark.read.parquet(f"{silver_path}/silver_Sales")
display(df_check)

