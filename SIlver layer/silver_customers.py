# -*- coding: utf-8 -*-
"""silver_customers.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1oFISORn6WYYZZqN5NJefDmY8Fg8z64gA
"""

# Welcome to your new notebook
# Type here in the cell editor to add code!

df = spark.read.format("csv").option("header","true").load("Files/Bronzebronze_customer_master/customer_master.csv")
# df now is a Spark DataFrame containing CSV data from "Files/Bronzebronze_customer_master/customer_master.csv".
display(df)

from pyspark.sql.functions import col, trim, upper, initcap

df_cust_silver = (
    df
    .withColumn("CustomerID", trim(upper(col("CustomerID"))))
    .withColumn("CustomerName", initcap(trim(col("CustomerName"))))
    .withColumn("Region", initcap(trim(col("Region"))))
    .withColumn("RiskCategory", initcap(trim(col("RiskCategory"))))
)

from pyspark.sql.functions import when

df_cust_silver = df_cust_silver.withColumn(
    "RiskCategory",
    when(col("RiskCategory").isNull(), "Unknown")
    .otherwise(col("RiskCategory"))
)

from pyspark.sql.types import IntegerType, DecimalType

df_cust_silver = (
    df_cust_silver
    .withColumn("CreditLimit", col("CreditLimit").cast(DecimalType(18,2)))
    .withColumn("CreditDays", col("CreditDays").cast(IntegerType()))
)

df_cust_silver = df_cust_silver.dropDuplicates(["CustomerID"])

from pyspark.sql.functions import monotonically_increasing_id

df_cust_silver = df_cust_silver.withColumn(
    "CustomerKey",
    monotonically_increasing_id()
)

from pyspark.sql.window import Window
from pyspark.sql.functions import row_number

window_spec = Window.orderBy("CustomerID")

df_cust_silver = df_cust_silver.withColumn(
    "CustomerKey",
    row_number().over(window_spec)
)

display(df_cust_silver)

from pyspark.sql.functions import current_date, lit

df_cust_silver = (
    df_cust_silver
    .withColumn("EffectiveFrom", current_date())
    .withColumn("EffectiveTo", lit(None).cast("date"))
    .withColumn("IsCurrent", lit(1))
)

df_cust_silver.printSchema()
display(df_cust_silver)

silver_path = "abfss://Fabric_Test@onelake.dfs.fabric.microsoft.com/Test_Lakehouse.Lakehouse/Files/Silver Layer Files"
df_cust_silver.write.mode("overwrite") \
    .parquet(f"{silver_path}/silver_customers")

df_check = spark.read.parquet(f"{silver_path}/silver_customers")
display(df_check)

